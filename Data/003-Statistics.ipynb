{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 1: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Types of Statistics\n",
    "- Descriptive Statistics - describe and summarizes the data\n",
    "- Inferential Statistics -  use of data to make inferences about a larger population\n",
    "\n",
    "Types of Data\n",
    "- Numeric (Quantitative)\n",
    "    - Continuous: Measured - Airplane, Time Spent\n",
    "    - Discrete: Counted - Number of Pets, Number of Packages Shipped\n",
    "\n",
    "- Categorical (Qualitative)\n",
    "    - Nominal: Unordered\n",
    "    - Ordinal: Ordered\n",
    "    \n",
    "    \n",
    "Continuous vs Discrete vs Categorical\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "The field of statistics - the practice or study of collecting and analyzing data\n",
    "Summary Statistics - a fact about or summary of some data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy with alias np\n",
    "import numpy as np\n",
    "\n",
    "food_consumption = None\n",
    "\n",
    "# Filter for Belgium\n",
    "be_consumption = food_consumption[food_consumption['country'] == 'Belgium']\n",
    "\n",
    "# Filter for USA\n",
    "usa_consumption = food_consumption[food_consumption['country'] == 'USA']\n",
    "\n",
    "# Calculate mean and median consumption in Belgium\n",
    "print(np.mean(be_consumption['consumption']))\n",
    "print(np.median(be_consumption['consumption']))\n",
    "\n",
    "# Calculate mean and median consumption in USA\n",
    "print(np.mean(usa_consumption['consumption']))\n",
    "print(np.median(usa_consumption['consumption']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "# Subset for Belgium and USA only\n",
    "be_and_usa = food_consumption[(food_consumption['country'] == 'Belgium') | (food_consumption['country'] == 'USA')]\n",
    "\n",
    "# Group by country, select consumption column, and compute mean and median\n",
    "print(be_and_usa.groupby(\"country\")[\"consumption\"].agg([np.mean, np.median]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the Spread of the data\n",
    "# - Standard Deviation: np.std(df[\"columns\"], ddof=1)\n",
    "# - Variance: np.var(df[\"columns\"], ddof=1)\n",
    "# - Mean Absolute Deviation\n",
    "\n",
    "# standard deviation squares distances, penalizing longer distances more than shorter ones.\n",
    "# Standard Deviation is more common other than Mean Absolute Deviate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantiles\n",
    "# Interquartile Range (IQR)\n",
    "# Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None\n",
    "\n",
    "# Quantiles\n",
    "np.quantile(df[\"columns\"], 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure of Spread (Location based within the data)\n",
    "\n",
    "Measures how apart or close together the data points are.\n",
    "- Variance\n",
    "- Standard Deviation\n",
    "- Mean Absolute Deviation\n",
    "\n",
    "- Quantiles | Percentiles\n",
    "- Interquartile Range (IQR)\n",
    "- Outliers\n",
    "\n",
    "Note\n",
    "- Standard Deviation is more common than Mean Absolute Deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance on a sample population\n",
    "np.var(df[\"columns\"], ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Deviation\n",
    "np.std(df[\"columns\"], ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Deviation\n",
    "# dists = msleep[\"sleep_total\"] - mean(msleep$sleep_total)\n",
    "# np.mean(np.abs(dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantiles\n",
    "np.quantile(df[\"column\"], 0.5)  # Same as the median\n",
    "np.quantile(df[\"column\"], [0,0.25,0.5,0.75,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interquartile Range\n",
    "np.quantile(df[\"column\"], 0.75) - np.quantile(df[\"column\"], 0.25)\n",
    "\n",
    "from scipy.stats import iqr\n",
    "iqr(df[\"column\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outliers - data point that is substantially different from the others\n",
    "# Formula\n",
    "# data < Q1 - 1.5 x IQR or data > Q3 + 1.5 x IQR\n",
    "\n",
    "# Process\n",
    "# - calculate the IQR\n",
    "# - calculate the lower threshold\n",
    "# - calculate the upper threshold\n",
    "# - subset or slice the dataframe to get the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the IQR\n",
    "from scipy.stats import iqr\n",
    "iqr = iqr(df[\"columns\"])\n",
    "\n",
    "# Calculate the upper and lower threshold\n",
    "lower_threshold = np.quantile(df[\"columns\"], 0.25) - 1.5 * iqr\n",
    "upper_threshold = np.quantile(df[\"columns\"], 0.75) + 1.5 * iqr\n",
    "\n",
    "# Subset the data (Slicing the data)\n",
    "df[(df[\"columns\"]<lower_threshold) | (df[\"columns\"]>upper_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating everything\n",
    "df[\"columns\"].describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n",
       "       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.linspace(0, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE\n",
    "\n",
    "# Print variance and sd of co2_emission for each food_category\n",
    "print(food_consumption.groupby('food_category')['co2_emission'].agg([np.var, np.std]))\n",
    "\n",
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create histogram of co2_emission for food_category 'beef'\n",
    "food_consumption[food_consumption['food_category'] == 'beef']['co2_emission'].hist()\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Create histogram of co2_emission for food_category 'eggs'\n",
    "food_consumption[food_consumption['food_category'] == 'eggs']['co2_emission'].hist()\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 2: Random Numbers and Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With or Without Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the deals for each product\n",
    "product_count = df[\"product\"].value_count()\n",
    "\n",
    "# Calculate the probability of picking a deal with each product\n",
    "product_probability = product_count / df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the random seed\n",
    "np.random.seed(24)\n",
    "\n",
    "# Sample 5 deals without replacement\n",
    "sample = df.sample(5)\n",
    "\n",
    "# Sample 5 deals with replacement\n",
    "sample = df.sample(5, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discrete Distribution - Video\n",
    "1/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from Discrete Distribution\n",
    "die = None\n",
    "rolls_10 = None\n",
    "\n",
    "die.sample(10, replace=True)\n",
    "\n",
    "# Visualizing the die sample\n",
    "rolls_10[\"number\"].hist(bins=np.linspace(1,7,7))\n",
    "\n",
    "# Law of Large Number - as the size of your sample increases, the sample\\\n",
    "    # mean will approach the expected value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from Discrete Distribution\n",
    "\n",
    "# # Create probability distribution\n",
    "# size_dist = restaurant_groups['group_size'].value_counts() / restaurant_groups.shape[0]\n",
    "\n",
    "# # Reset index and rename columns\n",
    "# size_dist = size_dist.reset_index()\n",
    "# size_dist.columns = ['group_size', 'prob']\n",
    "\n",
    "# # Expected value\n",
    "# expected_value = np.sum(size_dist['group_size'] * size_dist['prob'])\n",
    "\n",
    "# # Subset groups of size 4 or more\n",
    "# groups_4_or_more = size_dist[size_dist['group_size'] >= 4]\n",
    "\n",
    "# # Sum the probabilities of groups_4_or_more\n",
    "# prob_4_or_more = np.sum(groups_4_or_more['prob'])\n",
    "# print(prob_4_or_more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling from Continuous Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Types of distribution.\n",
    "- Discrete\n",
    "- Continuous\n",
    "- Binomial \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F_onewayBadInputSizesWarning',\n",
       " 'F_onewayConstantInputWarning',\n",
       " 'PearsonRConstantInputWarning',\n",
       " 'PearsonRNearConstantInputWarning',\n",
       " 'SpearmanRConstantInputWarning',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_binned_statistic',\n",
       " '_constants',\n",
       " '_continuous_distns',\n",
       " '_discrete_distns',\n",
       " '_distn_infrastructure',\n",
       " '_distr_params',\n",
       " '_hypotests',\n",
       " '_ksstats',\n",
       " '_multivariate',\n",
       " '_rvs_sampling',\n",
       " '_stats',\n",
       " '_stats_mstats_common',\n",
       " '_tukeylambda_stats',\n",
       " '_wilcoxon_data',\n",
       " 'alpha',\n",
       " 'anderson',\n",
       " 'anderson_ksamp',\n",
       " 'anglit',\n",
       " 'ansari',\n",
       " 'arcsine',\n",
       " 'argus',\n",
       " 'bartlett',\n",
       " 'bayes_mvs',\n",
       " 'bernoulli',\n",
       " 'beta',\n",
       " 'betabinom',\n",
       " 'betaprime',\n",
       " 'binned_statistic',\n",
       " 'binned_statistic_2d',\n",
       " 'binned_statistic_dd',\n",
       " 'binom',\n",
       " 'binom_test',\n",
       " 'boltzmann',\n",
       " 'boxcox',\n",
       " 'boxcox_llf',\n",
       " 'boxcox_normmax',\n",
       " 'boxcox_normplot',\n",
       " 'bradford',\n",
       " 'brunnermunzel',\n",
       " 'burr',\n",
       " 'burr12',\n",
       " 'cauchy',\n",
       " 'chi',\n",
       " 'chi2',\n",
       " 'chi2_contingency',\n",
       " 'chisquare',\n",
       " 'circmean',\n",
       " 'circstd',\n",
       " 'circvar',\n",
       " 'combine_pvalues',\n",
       " 'contingency',\n",
       " 'cosine',\n",
       " 'cramervonmises',\n",
       " 'crystalball',\n",
       " 'cumfreq',\n",
       " 'describe',\n",
       " 'dgamma',\n",
       " 'dirichlet',\n",
       " 'distributions',\n",
       " 'dlaplace',\n",
       " 'dweibull',\n",
       " 'energy_distance',\n",
       " 'entropy',\n",
       " 'epps_singleton_2samp',\n",
       " 'erlang',\n",
       " 'expon',\n",
       " 'exponnorm',\n",
       " 'exponpow',\n",
       " 'exponweib',\n",
       " 'f',\n",
       " 'f_oneway',\n",
       " 'fatiguelife',\n",
       " 'find_repeats',\n",
       " 'fisher_exact',\n",
       " 'fisk',\n",
       " 'fligner',\n",
       " 'foldcauchy',\n",
       " 'foldnorm',\n",
       " 'friedmanchisquare',\n",
       " 'gamma',\n",
       " 'gausshyper',\n",
       " 'gaussian_kde',\n",
       " 'genexpon',\n",
       " 'genextreme',\n",
       " 'gengamma',\n",
       " 'genhalflogistic',\n",
       " 'geninvgauss',\n",
       " 'genlogistic',\n",
       " 'gennorm',\n",
       " 'genpareto',\n",
       " 'geom',\n",
       " 'gilbrat',\n",
       " 'gmean',\n",
       " 'gompertz',\n",
       " 'gstd',\n",
       " 'gumbel_l',\n",
       " 'gumbel_r',\n",
       " 'halfcauchy',\n",
       " 'halfgennorm',\n",
       " 'halflogistic',\n",
       " 'halfnorm',\n",
       " 'hmean',\n",
       " 'hypergeom',\n",
       " 'hypsecant',\n",
       " 'invgamma',\n",
       " 'invgauss',\n",
       " 'invweibull',\n",
       " 'invwishart',\n",
       " 'iqr',\n",
       " 'itemfreq',\n",
       " 'jarque_bera',\n",
       " 'johnsonsb',\n",
       " 'johnsonsu',\n",
       " 'kappa3',\n",
       " 'kappa4',\n",
       " 'kde',\n",
       " 'kendalltau',\n",
       " 'kruskal',\n",
       " 'ks_1samp',\n",
       " 'ks_2samp',\n",
       " 'ksone',\n",
       " 'kstat',\n",
       " 'kstatvar',\n",
       " 'kstest',\n",
       " 'kstwo',\n",
       " 'kstwobign',\n",
       " 'kurtosis',\n",
       " 'kurtosistest',\n",
       " 'laplace',\n",
       " 'laplace_asymmetric',\n",
       " 'levene',\n",
       " 'levy',\n",
       " 'levy_l',\n",
       " 'levy_stable',\n",
       " 'linregress',\n",
       " 'loggamma',\n",
       " 'logistic',\n",
       " 'loglaplace',\n",
       " 'lognorm',\n",
       " 'logser',\n",
       " 'loguniform',\n",
       " 'lomax',\n",
       " 'mannwhitneyu',\n",
       " 'matrix_normal',\n",
       " 'maxwell',\n",
       " 'median_abs_deviation',\n",
       " 'median_absolute_deviation',\n",
       " 'median_test',\n",
       " 'mielke',\n",
       " 'mode',\n",
       " 'moment',\n",
       " 'mood',\n",
       " 'morestats',\n",
       " 'moyal',\n",
       " 'mstats',\n",
       " 'mstats_basic',\n",
       " 'mstats_extras',\n",
       " 'multinomial',\n",
       " 'multiscale_graphcorr',\n",
       " 'multivariate_hypergeom',\n",
       " 'multivariate_normal',\n",
       " 'multivariate_t',\n",
       " 'mvn',\n",
       " 'mvsdist',\n",
       " 'nakagami',\n",
       " 'nbinom',\n",
       " 'ncf',\n",
       " 'nct',\n",
       " 'ncx2',\n",
       " 'nhypergeom',\n",
       " 'norm',\n",
       " 'normaltest',\n",
       " 'norminvgauss',\n",
       " 'obrientransform',\n",
       " 'ortho_group',\n",
       " 'pareto',\n",
       " 'pearson3',\n",
       " 'pearsonr',\n",
       " 'percentileofscore',\n",
       " 'planck',\n",
       " 'pointbiserialr',\n",
       " 'poisson',\n",
       " 'power_divergence',\n",
       " 'powerlaw',\n",
       " 'powerlognorm',\n",
       " 'powernorm',\n",
       " 'ppcc_max',\n",
       " 'ppcc_plot',\n",
       " 'probplot',\n",
       " 'randint',\n",
       " 'random_correlation',\n",
       " 'rankdata',\n",
       " 'ranksums',\n",
       " 'rayleigh',\n",
       " 'rdist',\n",
       " 'recipinvgauss',\n",
       " 'reciprocal',\n",
       " 'relfreq',\n",
       " 'rice',\n",
       " 'rv_continuous',\n",
       " 'rv_discrete',\n",
       " 'rv_histogram',\n",
       " 'rvs_ratio_uniforms',\n",
       " 'scoreatpercentile',\n",
       " 'sem',\n",
       " 'semicircular',\n",
       " 'shapiro',\n",
       " 'siegelslopes',\n",
       " 'sigmaclip',\n",
       " 'skellam',\n",
       " 'skew',\n",
       " 'skewnorm',\n",
       " 'skewtest',\n",
       " 'spearmanr',\n",
       " 'special_ortho_group',\n",
       " 'statlib',\n",
       " 'stats',\n",
       " 't',\n",
       " 'test',\n",
       " 'theilslopes',\n",
       " 'tiecorrect',\n",
       " 'tmax',\n",
       " 'tmean',\n",
       " 'tmin',\n",
       " 'trapezoid',\n",
       " 'trapz',\n",
       " 'triang',\n",
       " 'trim1',\n",
       " 'trim_mean',\n",
       " 'trimboth',\n",
       " 'truncexpon',\n",
       " 'truncnorm',\n",
       " 'tsem',\n",
       " 'tstd',\n",
       " 'ttest_1samp',\n",
       " 'ttest_ind',\n",
       " 'ttest_ind_from_stats',\n",
       " 'ttest_rel',\n",
       " 'tukeylambda',\n",
       " 'tvar',\n",
       " 'uniform',\n",
       " 'unitary_group',\n",
       " 'variation',\n",
       " 'vonmises',\n",
       " 'vonmises_line',\n",
       " 'wald',\n",
       " 'wasserstein_distance',\n",
       " 'weibull_max',\n",
       " 'weibull_min',\n",
       " 'weightedtau',\n",
       " 'wilcoxon',\n",
       " 'wishart',\n",
       " 'wrapcauchy',\n",
       " 'yeojohnson',\n",
       " 'yeojohnson_llf',\n",
       " 'yeojohnson_normmax',\n",
       " 'yeojohnson_normplot',\n",
       " 'yulesimon',\n",
       " 'zipf',\n",
       " 'zmap',\n",
       " 'zscore']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
